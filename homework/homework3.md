# Search and Run

计54 马子轩

#### 题目1

大家考虑一下，如果一个搜索引擎需要支持短语查询（Phrase Search），在建立索引以及进行检索的时候，需要做什么样的扩展。主要是需要考虑各种情况，尽可能列举你认为在实际当中会出现的情况。这里的短语查询是在搜索引擎上包含在引号的内容。例如”Tsinghua University”，这样需要检索出来的就是包括这两个单词的所有的文档，并且这两个单词还需要相邻。当然，这是两个单词组成短语的情况，我们还可以支持任意长度的短语的查询。（如何使用MapReduce进行实现？）

短语搜索可以通过倒排列表来完成. 像lab1一样完成一个单词到所在文件-所在第几个单词的列表.

在搜索的时候, 按照candidate-validate的方式进行搜索. 优先使用候选最少的单词进行搜索. 并根据结果对其他单词的列表进行筛选即可.

```
search: A B C
A: c1-3 c1-6 c2-2 c3-2
B: c1-7 c2-3 c3-1
C: c1-5 c1-8

pick C
A: c1-3(o) c1-6(o) c2-2(x) c3-2(x)
B: c1-7(o) c2-3(x) c3-1(x)

pick B
A: c1-3(x) c1-6(o)
即可选出c1-6->7->8为A B C
```



#### 题目2

大家现在已经有了MapReduce程序的运行的基本直观感受。假设底层有了HDFS这样的一个文件系统，上层有一个总控，能够看到集群中的各个节点的程序执行情况，资源使用情况。然后我们通过hadoop可以提交一个jar文件到集群中执行（如1000台机器）。各位按照自己的理解，勾画出从提交开始，到执行完成这个MapReduce任务的全流程（可以以WordCount为例）。（可以参考一下论文，但是需要结合各位使用HDFS的感受，把这个过程具体化一下。）注意：命令行启动的时候就启动了一个java虚拟机，而虚拟机会去执行jar文件。在集群中的每一个机器在执行新任务的时候，都需要启动新的java虚拟机来执行，并且jar文件只有保存在本地文件系统的时候，才能够被java虚拟机所使用。

第一步是在main中设置job的各个属性并传参. 之后map阶段. 先进行input format. 对文件进行分块. 得到大小合适的内容放进mapper. mapper先setup context. 然后使用record reader一个一个读取key-value对. 通过map函数执行这一步. 之后调用combiner 进行combine操作. 在本机内通过combiner来减少reduce的工作量. 之后进行shuffle操作得到key到values的组合. 进入reducer. reducer和mapper先进行setup context然后依次按照key调用reduce函数. 最后写入context. 结束运行过程返回.